{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan of action.\n",
    "We have too little data to train a model dircetly on the data. It will either be too stupid or terribly overfit.\n",
    "\n",
    "To work with this, we will use different data, which is label with that slighly correspond to the labels we are interested in. We will train the net, then remove the last layers then and add new layers ontop. Then finetune.\n",
    "\n",
    "Ideas:\n",
    "1) don't use pooling but stride to reduce amount of parameters.\n",
    "2) normailze\n",
    "3) batch norm\n",
    "4) dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import GetData\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "importlib.reload(GetData)\n",
    "\n",
    "train_loader, test_loader = GetData.load_pretrain_data(batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda.0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "def train_model(model, data_loader, epochs, lr=0.1, optim=None):\n",
    "  if optim is None:\n",
    "    optimizer = torch.optim.SGD(params=model.parameters(), lr=lr)\n",
    "  else:\n",
    "    optimizer = optim\n",
    "  loss_fn = nn.MSELoss()\n",
    "  \n",
    "  model.to(device)\n",
    "  for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "      X = X.to(device)\n",
    "      y = y.to(device)\n",
    "      y_pred = model(X)\n",
    "      loss = loss_fn(y_pred, y)\n",
    "      epoch_loss += loss.item()\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    print(f\"average batch loss in {epoch+1}: {epoch_loss/len(data_loader)}\")\n",
    "# test loop\n",
    "def test_model(model, data_loader):\n",
    "  loss_fn = nn.MSELoss() \n",
    "  model.to(device)\n",
    "  with torch.no_grad():\n",
    "    for batch, (X,y) in enumerate(data_loader):\n",
    "      X = X.to(device)\n",
    "      y = y.to(device)\n",
    "      y_pred = model(X)\n",
    "      loss = loss_fn(y_pred, y)\n",
    "    print(f\"average batch loss: {loss.item()/len(data_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=6, stride=2, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=6, out_channels=16, stride=2, kernel_size=3)\n",
    "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=120, stride=2, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(120*124, 84)\n",
    "        self.fc2 = nn.Linear(84, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze_(dim=1)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(-1, 120*124)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.squeeze_(dim=1)\n",
    "        return x\n",
    "    \n",
    "# using batch normalization\n",
    "class normalized_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=6, stride=2, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm1d(6)\n",
    "        self.conv2 = nn.Conv1d(in_channels=6, out_channels=16, stride=2, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=120, stride=2, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(120)\n",
    "        self.fc1 = nn.Linear(120*124, 84)\n",
    "        self.fc2 = nn.Linear(84, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze_(dim=1) # need the x = x.unsqueeze_(dim=1) so gradient computation works\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.view(-1, 120*124)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.squeeze_(dim=1)    \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "test = next(iter(train_loader))[0]\n",
    "dev_model = net()\n",
    "out = dev_model(test)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_model = net()\n",
    "test_model(dev_model, train_loader)\n",
    "train_model(dev_model, train_loader, epochs=10, lr=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
