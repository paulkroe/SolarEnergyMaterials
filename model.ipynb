{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2PR5YdnJK0x"
      },
      "source": [
        "## Plan of action.\n",
        "We have too little data to train a model dircetly on the data. It will either be too stupid or terribly overfit.\n",
        "\n",
        "To work with this, we will use different data, which is label with that slighly correspond to the labels we are interested in. We will train the net, then remove the last layers then and add new layers ontop. Then finetune.\n",
        "\n",
        "Ideas:\n",
        "1. don't use pooling but stride to reduce amount of parameters.\n",
        "2. normailze\n",
        "3. batch norm\n",
        "4. dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OawkIaS3JK0z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt3yrRBbJK0z",
        "outputId": "15324536-f6b4-4722-c732-7cddd700f93d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get data"
      ],
      "metadata": {
        "id": "yaadEg-DJjJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5VpIMVrL2Do",
        "outputId": "fd05093c-ffeb-4440-a533-03c921beab8a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.unpack_archive(\"drive/MyDrive/SolarEnergyMaterials/task4_hr35z9.zip\", \"/content/data\")\n",
        "shutil.unpack_archive(\"data/task4_hr35z9/pretrain_features.csv.zip\", \"/content/data\")\n",
        "shutil.unpack_archive(\"data/task4_hr35z9/pretrain_labels.csv.zip\", \"/content/data\")"
      ],
      "metadata": {
        "id": "IrDmZKsCJqao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "EPSILON = 1e-10\n",
        "def load_pretrain_data(batch_size = 64):\n",
        "    batch_size = 64\n",
        "\n",
        "    random.seed(17)\n",
        "    test_ind = set()\n",
        "\n",
        "    pre_train_size = 50000\n",
        "\n",
        "    while len(test_ind) < 10000: \n",
        "        test_ind.add(random.randint(0, pre_train_size-1))\n",
        "\n",
        "    features =[]\n",
        "    labels = []\n",
        "\n",
        "    with open(\"data/pretrain_features.csv\", 'r') as f:\n",
        "        for row in f:\n",
        "            features.append(row)\n",
        "\n",
        "    with open(\"data/pretrain_labels.csv\", 'r') as f:\n",
        "        for row in f:\n",
        "            labels.append(row)\n",
        "\n",
        "    # remove header\n",
        "    features = features[1:]\n",
        "    labels = labels[1:]\n",
        "\n",
        "    # first try to note use representation of the molecules, only the extracted features\n",
        "    features = [list(map(float,row.split(',')[2:])) for row in features]\n",
        "    labels = [float(row.split(',')[1]) for row in labels]\n",
        "\n",
        "    train_features = []\n",
        "    train_labels = []\n",
        "    test_features = []\n",
        "    test_labels = []\n",
        "\n",
        "\n",
        "    for i in range(len(features)):\n",
        "        if i in test_ind:\n",
        "            test_features.append(features[i])\n",
        "            test_labels.append(labels[i])\n",
        "        else:\n",
        "            train_features.append(features[i])\n",
        "            train_labels.append(labels[i])\n",
        "\n",
        "    # does not seem to make sense to normalize the data since it is very sparse\n",
        "    # normalize train_features\n",
        "    # train_features = (train_features - np.mean(train_features, axis=0)) / (np.std(train_features, axis=0)+EPSILON)\n",
        "\n",
        "    # normalize test_features\n",
        "    # test_features = (test_features - np.mean(test_features, axis=0)) / (np.std(test_features, axis=0)+EPSILON)\n",
        "\n",
        "    # convert into tensor dataset\n",
        "    train_features = torch.tensor(train_features, dtype=torch.float)\n",
        "    train_labels = torch.tensor(train_labels, dtype=torch.float)\n",
        "    test_features = torch.tensor(test_features, dtype=torch.float)\n",
        "    test_labels = torch.tensor(test_labels, dtype=torch.float)\n",
        "\n",
        "    train_dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
        "    test_dataset = torch.utils.data.TensorDataset(test_features, test_labels) \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "-Vp66MeOJksf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, test_loader = load_pretrain_data(batch_size = 64)"
      ],
      "metadata": {
        "id": "gNKQIcF5N0WE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train model\n"
      ],
      "metadata": {
        "id": "hjnNgtwKJnM_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "69MXrw4lJK00"
      },
      "outputs": [],
      "source": [
        "# train loop\n",
        "def train_model(model, data_loader, epochs, lr=0.1, optim=None):\n",
        "  if optim is None:\n",
        "    optimizer = torch.optim.SGD(params=model.parameters(), lr=lr)\n",
        "  else:\n",
        "    optimizer = optim\n",
        "  loss_fn = nn.MSELoss()\n",
        "  \n",
        "  model.to(device)\n",
        "  for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      y_pred = model(X)\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      epoch_loss += loss.item()\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print(f\"average batch loss in {epoch+1}: {epoch_loss/len(data_loader)}\")\n",
        "# test loop\n",
        "def test_model(model, data_loader):\n",
        "  loss_fn = nn.MSELoss() \n",
        "  model.to(device)\n",
        "  with torch.no_grad():\n",
        "    for batch, (X,y) in enumerate(data_loader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      y_pred = model(X)\n",
        "      loss = loss_fn(y_pred, y)\n",
        "    print(f\"average batch loss: {loss.item()/len(data_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_skh3Z1rJK01"
      },
      "outputs": [],
      "source": [
        "class net(nn.Module):\n",
        "    def __init__(self):\n",
        "        torch.manual_seed = 17\n",
        "        super(net, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=6, stride=2, kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(in_channels=6, out_channels=16, stride=2, kernel_size=3)\n",
        "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=120, stride=2, kernel_size=3)\n",
        "        self.fc1 = nn.Linear(120*124, 84)\n",
        "        self.fc2 = nn.Linear(84, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze_(dim=1)\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = x.view(-1, 120*124)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = x.squeeze_(dim=1)\n",
        "        return x\n",
        "    \n",
        "# using batch normalization\n",
        "class normalized_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        torch.manual_seed = 17\n",
        "        super(normalized_net, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=6, stride=2, kernel_size=3)\n",
        "        self.bn1 = nn.BatchNorm1d(6)\n",
        "        self.conv2 = nn.Conv1d(in_channels=6, out_channels=16, stride=2, kernel_size=3)\n",
        "        self.bn2 = nn.BatchNorm1d(16)\n",
        "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=120, stride=2, kernel_size=3)\n",
        "        self.bn3 = nn.BatchNorm1d(120)\n",
        "        self.fc1 = nn.Linear(120*124, 84)\n",
        "        self.fc2 = nn.Linear(84, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze_(dim=1) # need the x = x.unsqueeze_(dim=1) so gradient computation works\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = torch.relu(self.bn3(self.conv3(x)))\n",
        "        x = x.view(-1, 120*124)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = x.squeeze_(dim=1)    \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wf7yw5xYJK01",
        "outputId": "4521ac55-da16-4ee9-e98f-27a454795831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "test = next(iter(train_loader))[0]\n",
        "dev_model = net()\n",
        "out = dev_model(test)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aLdYUbNJK01",
        "outputId": "33f406da-1520-4a3e-999a-2cb1f7aeb97e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 1: 0.8780323009103537\n",
            "average batch loss in 2: 0.029272877728939058\n",
            "average batch loss in 3: 0.026178868517279626\n",
            "average batch loss in 4: 0.021420786726474763\n",
            "average batch loss in 5: 0.020250629971921445\n",
            "average batch loss in 6: 0.01895062441378832\n",
            "average batch loss in 7: 0.018103766920417548\n",
            "average batch loss in 8: 0.01771333412900567\n",
            "average batch loss in 9: 0.017743518847227097\n",
            "average batch loss in 10: 0.016926711418479682\n",
            "average batch loss in 11: 0.014926233030110597\n",
            "average batch loss in 12: 0.014913811583817006\n",
            "average batch loss in 13: 0.013814114114642144\n",
            "average batch loss in 14: 0.013071136762201787\n",
            "average batch loss in 15: 0.01182014731541276\n",
            "---\n",
            "average batch loss: 1.305096596479416e-05\n",
            "---\n",
            "average batch loss: 0.00011510908556212286\n"
          ]
        }
      ],
      "source": [
        "dev_model =  normalized_net()\n",
        "train_model(dev_model, train_loader, epochs=15, lr=0.01)\n",
        "print('---')\n",
        "test_model(dev_model, train_loader)\n",
        "print('---')\n",
        "test_model(dev_model, test_loader)\n",
        "# does not really generalize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_model =  normalized_net()\n",
        "train_model(dev_model, train_loader, epochs=15, lr=0.01)\n",
        "print('---')\n",
        "test_model(dev_model, train_loader)\n",
        "print('---')\n",
        "test_model(dev_model, test_loader)\n",
        "# does not really generalize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuefK9EMOPJi",
        "outputId": "d12f98b0-75b5-4645-bbf2-50199bf8dbe7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 1: 0.8957869546920061\n",
            "average batch loss in 2: 0.021710731948912144\n",
            "average batch loss in 3: 0.018483167152106763\n",
            "average batch loss in 4: 0.01670166256353259\n",
            "average batch loss in 5: 0.014878908500820399\n",
            "average batch loss in 6: 0.013457754176855087\n",
            "average batch loss in 7: 0.013481087016314269\n",
            "average batch loss in 8: 0.01195252608731389\n",
            "average batch loss in 9: 0.01113129082173109\n",
            "average batch loss in 10: 0.011459659742936492\n",
            "average batch loss in 11: 0.010420427816361188\n",
            "average batch loss in 12: 0.010572349233180284\n",
            "average batch loss in 13: 0.009954427929222584\n",
            "average batch loss in 14: 0.009936350903660058\n",
            "average batch loss in 15: 0.009829613841325045\n",
            "---\n",
            "average batch loss: 1.1551555991172791e-05\n",
            "---\n",
            "average batch loss: 6.329600408578375e-05\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}