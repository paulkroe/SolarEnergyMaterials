{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2PR5YdnJK0x"
      },
      "source": [
        "## Plan of action.\n",
        "Naive approach:\n",
        "\n",
        "We have too little data to train a model dircetly on the data. It will either be too stupid or terribly overfit.\n",
        "\n",
        "Ideas:\n",
        "1. don't use pooling but stride to reduce amount of parameters.\n",
        "2. normailze\n",
        "3. batch norm\n",
        "4. dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "OawkIaS3JK0z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt3yrRBbJK0z",
        "outputId": "f3f17e72-7685-4e2c-e11c-60a40823efe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get data"
      ],
      "metadata": {
        "id": "yaadEg-DJjJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5VpIMVrL2Do",
        "outputId": "adc1b61b-f53b-4ed8-d2e6-f0e38c66a363"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.unpack_archive(\"drive/MyDrive/SolarEnergyMaterials/task4.zip\", \"/content/data\")\n",
        "shutil.unpack_archive(\"data/task4_hr35z9/pretrain_features.csv.zip\", \"/content/data\")\n",
        "shutil.unpack_archive(\"data/task4_hr35z9/pretrain_labels.csv.zip\", \"/content/data\")\n",
        "shutil.unpack_archive(\"data/task4_hr35z9/train_features.csv.zip\", \"/content/data\")\n",
        "shutil.unpack_archive(\"data/task4_hr35z9/train_labels.csv.zip\", \"/content/data\")"
      ],
      "metadata": {
        "id": "IrDmZKsCJqao"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "EPSILON = 1e-10\n",
        "def load_pretrain_data(batch_size = 64):\n",
        "    batch_size = 64\n",
        "\n",
        "    random.seed(17)\n",
        "    test_ind = set()\n",
        "\n",
        "    pre_train_size = 50000\n",
        "\n",
        "    while len(test_ind) < 10000: \n",
        "        test_ind.add(random.randint(0, pre_train_size-1))\n",
        "\n",
        "    features =[]\n",
        "    labels = []\n",
        "\n",
        "    with open(\"data/pretrain_features.csv\", 'r') as f:\n",
        "        for row in f:\n",
        "            features.append(row)\n",
        "\n",
        "    with open(\"data/pretrain_labels.csv\", 'r') as f:\n",
        "        for row in f:\n",
        "            labels.append(row)\n",
        "\n",
        "    # remove header\n",
        "    features = features[1:]\n",
        "    labels = labels[1:]\n",
        "\n",
        "    # first try to note use representation of the molecules, only the extracted features\n",
        "    features = [list(map(float,row.split(',')[2:])) for row in features]\n",
        "    labels = [float(row.split(',')[1]) for row in labels]\n",
        "\n",
        "    train_features = []\n",
        "    train_labels = []\n",
        "    test_features = []\n",
        "    test_labels = []\n",
        "\n",
        "\n",
        "    for i in range(len(features)):\n",
        "        if i in test_ind:\n",
        "            test_features.append(features[i])\n",
        "            test_labels.append(labels[i])\n",
        "        else:\n",
        "            train_features.append(features[i])\n",
        "            train_labels.append(labels[i])\n",
        "\n",
        "    # does not seem to make sense to normalize the data since it is very sparse\n",
        "    # normalize train_features\n",
        "    # train_features = (train_features - np.mean(train_features, axis=0)) / (np.std(train_features, axis=0)+EPSILON)\n",
        "\n",
        "    # normalize test_features\n",
        "    # test_features = (test_features - np.mean(test_features, axis=0)) / (np.std(test_features, axis=0)+EPSILON)\n",
        "\n",
        "    # convert into tensor dataset\n",
        "    train_features = torch.tensor(train_features, dtype=torch.float)\n",
        "    train_labels = torch.tensor(train_labels, dtype=torch.float)\n",
        "    test_features = torch.tensor(test_features, dtype=torch.float)\n",
        "    test_labels = torch.tensor(test_labels, dtype=torch.float)\n",
        "\n",
        "    train_dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
        "    test_dataset = torch.utils.data.TensorDataset(test_features, test_labels) \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "-Vp66MeOJksf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, test_loader = load_pretrain_data(batch_size = 64)"
      ],
      "metadata": {
        "id": "gNKQIcF5N0WE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_finetune_data(batch_size = 4):\n",
        "    batch_size = 4\n",
        "\n",
        "    random.seed(17)\n",
        "    test_ind = set()\n",
        "\n",
        "    pre_train_size = 100\n",
        "    while len(test_ind) < 50: \n",
        "        test_ind.add(random.randint(0, pre_train_size-1))\n",
        "\n",
        "    features =[]\n",
        "    labels = []\n",
        "\n",
        "    with open(\"data/train_features.csv\", 'r') as f:\n",
        "        for row in f:\n",
        "            features.append(row)\n",
        "\n",
        "    with open(\"data/train_labels.csv\", 'r') as f:\n",
        "        for row in f:\n",
        "            labels.append(row)\n",
        "\n",
        "    # remove header\n",
        "    features = features[1:]\n",
        "    labels = labels[1:]\n",
        "\n",
        "    # first try to note use representation of the molecules, only the extracted features\n",
        "    features = [list(map(float,row.split(',')[2:])) for row in features]\n",
        "    labels = [float(row.split(',')[1]) for row in labels]\n",
        "\n",
        "    train_features = []\n",
        "    train_labels = []\n",
        "    test_features = []\n",
        "    test_labels = []\n",
        "\n",
        "\n",
        "    for i in range(len(features)):\n",
        "        if i in test_ind:\n",
        "            test_features.append(features[i])\n",
        "            test_labels.append(labels[i])\n",
        "        else:\n",
        "            train_features.append(features[i])\n",
        "            train_labels.append(labels[i])\n",
        "\n",
        "    # does not seem to make sense to normalize the data since it is very sparse\n",
        "    # normalize train_features\n",
        "    # train_features = (train_features - np.mean(train_features, axis=0)) / (np.std(train_features, axis=0)+EPSILON)\n",
        "\n",
        "    # normalize test_features\n",
        "    # test_features = (test_features - np.mean(test_features, axis=0)) / (np.std(test_features, axis=0)+EPSILON)\n",
        "\n",
        "    # convert into tensor dataset\n",
        "    train_features = torch.tensor(train_features, dtype=torch.float)\n",
        "    train_labels = torch.tensor(train_labels, dtype=torch.float)\n",
        "    test_features = torch.tensor(test_features, dtype=torch.float)\n",
        "    test_labels = torch.tensor(test_labels, dtype=torch.float)\n",
        "\n",
        "    train_dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
        "    test_dataset = torch.utils.data.TensorDataset(test_features, test_labels) \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "DHAQzZAM2h_Z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_train_loader, finetune_test_loader = load_finetune_data(batch_size = 4)"
      ],
      "metadata": {
        "id": "x65p7SBi266Q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train/test loop\n"
      ],
      "metadata": {
        "id": "hjnNgtwKJnM_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "69MXrw4lJK00"
      },
      "outputs": [],
      "source": [
        "# train loop\n",
        "def train_model(model, data_loader, epochs, lr=0.1, optim=None, weight_decay=None, p=True):\n",
        "  model.to(device)\n",
        "  if optim is None:\n",
        "    if weight_decay is None:\n",
        "      optimizer = torch.optim.SGD(params=model.parameters(), lr=lr)\n",
        "    else:\n",
        "      optimizer = torch.optim.SGD(params=model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "  else:\n",
        "    if weight_decay is None:\n",
        "      optimizer = optim(model.parameters(), lr=lr)\n",
        "    else:\n",
        "      optimizer = optim(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "  loss_fn = nn.MSELoss()\n",
        "  epoch_loss = []\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    epoch_loss.append(0)\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      y_pred = model(X)\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      epoch_loss[-1] += loss.item()\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    if p:\n",
        "      print(f\"average batch loss in {epoch+1}: {epoch_loss[-1]/len(data_loader)}\")\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "# test loop\n",
        "def test_model(model, data_loader):\n",
        "  loss_fn = nn.MSELoss() \n",
        "  model.to(device)\n",
        "  Y = torch.tensor([]).to(device)\n",
        "  Y_pred = torch.tensor([]).to(device)\n",
        "  with torch.no_grad():\n",
        "    for batch, (X,y) in enumerate(data_loader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      y_pred = model(X)\n",
        "      Y = torch.cat((Y, y))\n",
        "      Y_pred = torch.cat((Y_pred, y_pred))\n",
        "    loss = torch.sqrt(loss_fn(y_pred, y))\n",
        "    print(f\"average batch loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "_skh3Z1rJK01"
      },
      "outputs": [],
      "source": [
        "class net(nn.Module):\n",
        "    def __init__(self):\n",
        "        torch.manual_seed = 17\n",
        "        super(net, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=6, stride=2, kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(in_channels=6, out_channels=16, stride=2, kernel_size=3)\n",
        "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=120, stride=2, kernel_size=3)\n",
        "        self.fc1 = nn.Linear(120*124, 84)\n",
        "        self.fc2 = nn.Linear(84, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze_(dim=1)\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = x.view(-1, 120*124)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = x.squeeze_(dim=1)\n",
        "        return x\n",
        "    \n",
        "# using batch normalization\n",
        "class normalized_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        torch.manual_seed = 17\n",
        "        super(normalized_net, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=6, stride=2, kernel_size=3)\n",
        "        self.bn1 = nn.BatchNorm1d(6)\n",
        "        self.conv2 = nn.Conv1d(in_channels=6, out_channels=16, stride=2, kernel_size=3)\n",
        "        self.bn2 = nn.BatchNorm1d(16)\n",
        "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=32, stride=2, kernel_size=3)\n",
        "        self.bn3 = nn.BatchNorm1d(32)\n",
        "        self.conv4 = nn.Conv1d(in_channels=32, out_channels=64, stride=2, kernel_size=3)\n",
        "        self.bn4 = nn.BatchNorm1d(64)\n",
        "        self.conv5 = nn.Conv1d(in_channels=64, out_channels=128, stride=2, kernel_size=3)\n",
        "        self.bn5 = nn.BatchNorm1d(128)\n",
        "        self.fc1 = nn.Linear(128*30, 84)\n",
        "        self.fc2 = nn.Linear(84, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze_(dim=1) # need the x = x.unsqueeze_(dim=1) so gradient computation works\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = torch.relu(self.bn3(self.conv3(x)))\n",
        "        x = torch.relu(self.bn4(self.conv4(x)))\n",
        "        x = torch.relu(self.bn5(self.conv5(x)))\n",
        "        x = x.view(-1, 128*30)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = x.squeeze_(dim=1)    \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### comparing some hyperparamters"
      ],
      "metadata": {
        "id": "-mD4PtFa1pKh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wf7yw5xYJK01",
        "outputId": "29fb8658-4f7b-46c9-8067-cba260bac9a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 128, 30])\n"
          ]
        }
      ],
      "source": [
        "test = next(iter(train_loader))[0]\n",
        "dev_model = normalized_net()\n",
        "out = dev_model(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aLdYUbNJK01",
        "outputId": "e7110ad8-76b3-4ebe-e3d2-eeafa704f44e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 1: 0.04832211886197329\n",
            "average batch loss in 2: 0.015955748527497052\n",
            "average batch loss in 3: 0.0138593875028193\n",
            "average batch loss in 4: 0.013848679214715958\n",
            "average batch loss in 5: 0.013406210947781801\n",
            "average batch loss in 6: 0.013128400990366936\n",
            "average batch loss in 7: 0.013793663085997105\n",
            "average batch loss in 8: 0.013128213630616664\n",
            "average batch loss in 9: 0.013136810804903507\n",
            "average batch loss in 10: 0.01320596416592598\n",
            "average batch loss in 11: 0.013335453416407109\n",
            "average batch loss in 12: 0.012644454278796912\n",
            "average batch loss in 13: 0.012736681837588549\n",
            "average batch loss in 14: 0.012520046799629926\n",
            "average batch loss in 15: 0.012515485768765211\n",
            "---\n",
            "average batch loss: 1.4018318057060243e-05 | accuracy: 27602/40000 | accuracy in percent 69.005\n",
            "---\n",
            "average batch loss: 7.285112455771986e-05 | accuracy: 6773/10000 | accuracy in percent 67.73\n"
          ]
        }
      ],
      "source": [
        "dev_model =  normalized_net()\n",
        "train_model(dev_model, train_loader, epochs=15, optim = torch.optim.Adam, lr=0.001, weight_decay=0.002)\n",
        "print('---')\n",
        "test_model(dev_model, train_loader)\n",
        "print('---')\n",
        "test_model(dev_model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_model =  normalized_net()\n",
        "\n",
        "train_model(dev_model, train_loader, epochs=15, optim = torch.optim.Adagrad, lr=0.001, weight_decay=0.001)\n",
        "print('---')\n",
        "test_model(dev_model, train_loader)\n",
        "print('---')\n",
        "test_model(dev_model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7aRCRiAulqs",
        "outputId": "ec5dc731-4d2e-481a-cdec-280a7e6a2f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 1: 0.052456893715262416\n",
            "average batch loss in 2: 0.018803144995868205\n",
            "average batch loss in 3: 0.015965714767575263\n",
            "average batch loss in 4: 0.014213927049189806\n",
            "average batch loss in 5: 0.013095104674994945\n",
            "average batch loss in 6: 0.012271047741174698\n",
            "average batch loss in 7: 0.01156871896237135\n",
            "average batch loss in 8: 0.01102673379331827\n",
            "average batch loss in 9: 0.01059519100189209\n",
            "average batch loss in 10: 0.010167258009314537\n",
            "average batch loss in 11: 0.009883010215312243\n",
            "average batch loss in 12: 0.009588859386742116\n",
            "average batch loss in 13: 0.009299005978554487\n",
            "average batch loss in 14: 0.00901506588086486\n",
            "average batch loss in 15: 0.008847360903769731\n",
            "---\n",
            "average batch loss: 9.972722083330155e-06 | accuracy: 29922/40000 | accuracy in percent 74.805\n",
            "---\n",
            "average batch loss: 8.344114016575418e-05 | accuracy: 7031/10000 | accuracy in percent 70.31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_model =  normalized_net()\n",
        "train_model(dev_model, train_loader, epochs=15, lr=0.01)\n",
        "print('---')\n",
        "test_model(dev_model, train_loader)\n",
        "print('---')\n",
        "test_model(dev_model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuefK9EMOPJi",
        "outputId": "2cdce04b-2673-4d7c-e267-1439f5accff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 1: 0.4968699249774218\n",
            "average batch loss in 2: 0.03491322933137417\n",
            "average batch loss in 3: 0.02677759014368057\n",
            "average batch loss in 4: 0.022675443471968173\n",
            "average batch loss in 5: 0.01888897882774472\n",
            "average batch loss in 6: 0.01708962717205286\n",
            "average batch loss in 7: 0.014661124294251204\n",
            "average batch loss in 8: 0.014020600125193596\n",
            "average batch loss in 9: 0.013597100345045328\n",
            "average batch loss in 10: 0.012233984691649675\n",
            "average batch loss in 11: 0.012298735515773297\n",
            "average batch loss in 12: 0.012327293568104506\n",
            "average batch loss in 13: 0.01067632727175951\n",
            "average batch loss in 14: 0.010132453045248985\n",
            "average batch loss in 15: 0.010068597088754178\n",
            "---\n",
            "average batch loss: 9.880167245864868e-06 | accuracy: 30787/40000 | accuracy in percent 76.9675\n",
            "---\n",
            "average batch loss: 0.00010748181468362262 | accuracy: 7219/10000 | accuracy in percent 72.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_model =  normalized_net()\n",
        "train_model(dev_model, train_loader, epochs=15, lr=0.01)\n",
        "print('---')\n",
        "test_model(dev_model, train_loader)\n",
        "print('---')\n",
        "test_model(dev_model, test_loader)"
      ],
      "metadata": {
        "id": "pqbipy4XlZaW",
        "outputId": "393efa9b-e141-4374-e99a-1c850e624711",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 1: 0.4692110815644264\n",
            "average batch loss in 2: 0.03254912094771862\n",
            "average batch loss in 3: 0.024455007752776144\n",
            "average batch loss in 4: 0.02313570466041565\n",
            "average batch loss in 5: 0.020421098506450654\n",
            "average batch loss in 6: 0.01878229928314686\n",
            "average batch loss in 7: 0.017239975012093782\n",
            "average batch loss in 8: 0.014939844016730786\n",
            "average batch loss in 9: 0.013761905759572982\n",
            "average batch loss in 10: 0.013486493415385485\n",
            "average batch loss in 11: 0.012677794007211923\n",
            "average batch loss in 12: 0.01172583431005478\n",
            "average batch loss in 13: 0.011562794194370509\n",
            "average batch loss in 14: 0.01144865373969078\n",
            "average batch loss in 15: 0.010884522700309754\n",
            "---\n",
            "average batch loss: 0.1082547977566719\n",
            "---\n",
            "average batch loss: 0.10792825371026993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pretrain\n"
      ],
      "metadata": {
        "id": "zaNNxexynAwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model =  normalized_net()\n",
        "train_model(pretrained_model, train_loader, epochs=30, lr=0.01)\n",
        "print('---')\n",
        "test_model(pretrained_model, train_loader)\n",
        "print('---')\n",
        "test_model(pretrained_model, test_loader)\n",
        "\n"
      ],
      "metadata": {
        "id": "YfUO_bjW15xD",
        "outputId": "b266515b-8121-4c37-dacb-8f877916e815",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 1/30 [00:02<01:21,  2.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 1: 0.45400825462043287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 2/30 [00:05<01:10,  2.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 2: 0.03324480162411928\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 3/30 [00:07<01:03,  2.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 3: 0.026463318206369876\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 4/30 [00:09<00:59,  2.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 4: 0.018085084749758243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 5/30 [00:11<00:56,  2.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 5: 0.017630003628879787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 6/30 [00:13<00:54,  2.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 6: 0.016621013481169938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 7/30 [00:16<00:57,  2.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 7: 0.015120367111265659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 8/30 [00:18<00:52,  2.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 8: 0.014003087665885688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 9/30 [00:21<00:48,  2.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 9: 0.013488452656567097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 10/30 [00:23<00:44,  2.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 10: 0.012285828217864036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 11/30 [00:25<00:41,  2.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 11: 0.011755147004872561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 12/30 [00:27<00:40,  2.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 12: 0.010776967500895261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 13/30 [00:30<00:40,  2.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 13: 0.010149496012181044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 14/30 [00:32<00:37,  2.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 14: 0.0113375015437603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 15/30 [00:34<00:34,  2.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 15: 0.010579911043122411\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 16/30 [00:36<00:31,  2.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 16: 0.009368298490345478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 17/30 [00:38<00:28,  2.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 17: 0.008999977856129408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 18/30 [00:41<00:28,  2.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 18: 0.008866982467100024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 19/30 [00:44<00:27,  2.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 19: 0.008766584837436676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 20/30 [00:46<00:23,  2.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 20: 0.008470799985527992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 21/30 [00:48<00:20,  2.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 21: 0.008682735307142139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 22/30 [00:50<00:17,  2.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 22: 0.008284158957749605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 77%|███████▋  | 23/30 [00:53<00:15,  2.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 23: 0.0076819450981915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 24/30 [00:55<00:13,  2.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 24: 0.007597086875140667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 25/30 [00:58<00:11,  2.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 25: 0.00751025112643838\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 26/30 [01:00<00:09,  2.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 26: 0.007210653605312109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 27/30 [01:02<00:06,  2.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 27: 0.007294732500612736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 28/30 [01:04<00:04,  2.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 28: 0.006886829763650894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 29/30 [01:06<00:02,  2.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 29: 0.006887848288938403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [01:09<00:00,  2.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss in 30: 0.006635884273797274\n",
            "---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss: 0.09174182265996933\n",
            "---\n",
            "average batch loss: 0.10016132891178131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(pretrained_model.state_dict(), 'drive/MyDrive/SolarEnergyMaterials/PretrainedModels/model0.pth')"
      ],
      "metadata": {
        "id": "UJsTz3DwljMA"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### finetune\n"
      ],
      "metadata": {
        "id": "F5LqpZze1u5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class pretrained_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        torch.manual_seed = 17\n",
        "        super(pretrained_net, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=6, stride=2, kernel_size=3)\n",
        "        self.bn1 = nn.BatchNorm1d(6)\n",
        "        self.conv2 = nn.Conv1d(in_channels=6, out_channels=16, stride=2, kernel_size=3)\n",
        "        self.bn2 = nn.BatchNorm1d(16)\n",
        "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=32, stride=2, kernel_size=3)\n",
        "        self.bn3 = nn.BatchNorm1d(32)\n",
        "        self.conv4 = nn.Conv1d(in_channels=32, out_channels=64, stride=2, kernel_size=3)\n",
        "        self.bn4 = nn.BatchNorm1d(64)\n",
        "        self.conv5 = nn.Conv1d(in_channels=64, out_channels=128, stride=2, kernel_size=3)\n",
        "        self.bn5 = nn.BatchNorm1d(128)\n",
        "        self.fc1 = nn.Linear(128*30, 84)\n",
        "        self.fc2 = nn.Linear(84, 1)\n",
        "        self.load_state_dict(torch.load('drive/MyDrive/SolarEnergyMaterials/PretrainedModels/model0.pth'),strict=True)\n",
        "        # freezing pretrained layers\n",
        "        for param in model.parameters():\n",
        "          param.requires_grad = False\n",
        "        self.fc = nn.Linear(128*30, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze_(dim=1) # need the x = x.unsqueeze_(dim=1) so gradient computation works\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = torch.relu(self.bn3(self.conv3(x)))\n",
        "        x = torch.relu(self.bn4(self.conv4(x)))\n",
        "        x = torch.relu(self.bn5(self.conv5(x)))\n",
        "        x = x.view(-1, 128*30)\n",
        "        x = self.fc(x)\n",
        "        x = x.squeeze_(dim=1)    \n",
        "        return x"
      ],
      "metadata": {
        "id": "LIxFLnRFn6I6"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = pretrained_net()\n",
        "train_model(model, finetune_train_loader, epochs=75, lr=0.0001, p=False)\n",
        "test_model(model,finetune_train_loader)\n",
        "test_model(model,finetune_test_loader)"
      ],
      "metadata": {
        "id": "yF_Q1FZ0nG0W",
        "outputId": "bce10108-952c-48ae-c6d3-d56d499114aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 75/75 [00:02<00:00, 27.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average batch loss: 0.06577936559915543\n",
            "average batch loss: 0.33720239996910095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}