{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learnig the representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependecies\n",
    "import importlib\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import RepresentationModels as RM\n",
    "importlib.reload(RepresentationModels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.unpack_archive(\"drive/MyDrive/SolarEnergyMaterials/task4.zip\", \"/content/data\")\n",
    "shutil.unpack_archive(\"data/task4_hr35z9/pretrain_features.csv.zip\", \"/content/data\")\n",
    "shutil.unpack_archive(\"data/task4_hr35z9/pretrain_labels.csv.zip\", \"/content/data\")\n",
    "shutil.unpack_archive(\"data/task4_hr35z9/train_features.csv.zip\", \"/content/data\")\n",
    "shutil.unpack_archive(\"data/task4_hr35z9/train_labels.csv.zip\", \"/content/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrain_data(batch_size = 64):\n",
    "    batch_size = 64\n",
    "\n",
    "    random.seed(17)\n",
    "    test_ind = set()\n",
    "\n",
    "    pre_train_size = 50000\n",
    "\n",
    "    while len(test_ind) < 10000: \n",
    "        test_ind.add(random.randint(0, pre_train_size-1))\n",
    "\n",
    "    features =[]\n",
    "    labels = []\n",
    "\n",
    "    with open(\"data/pretrain_features.csv\", 'r') as f:\n",
    "        for row in f:\n",
    "            features.append(row)\n",
    "\n",
    "    with open(\"data/pretrain_labels.csv\", 'r') as f:\n",
    "        for row in f:\n",
    "            labels.append(row)\n",
    "\n",
    "    # remove header\n",
    "    features = features[1:]\n",
    "    labels = labels[1:]\n",
    "\n",
    "    # first try to note use representation of the molecules, only the extracted features\n",
    "    features = [list(map(float,row.split(',')[2:])) for row in features]\n",
    "    labels = [float(row.split(',')[1]) for row in labels]\n",
    "\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    test_features = []\n",
    "    test_labels = []\n",
    "\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        if i in test_ind:\n",
    "            test_features.append(features[i])\n",
    "            test_labels.append(labels[i])\n",
    "        else:\n",
    "            train_features.append(features[i])\n",
    "            train_labels.append(labels[i])\n",
    "\n",
    "    # does not seem to make sense to normalize the data since it is very sparse\n",
    "    # normalize train_features\n",
    "    # train_features = (train_features - np.mean(train_features, axis=0)) / (np.std(train_features, axis=0)+EPSILON)\n",
    "\n",
    "    # normalize test_features\n",
    "    # test_features = (test_features - np.mean(test_features, axis=0)) / (np.std(test_features, axis=0)+EPSILON)\n",
    "\n",
    "    # convert into tensor dataset\n",
    "    train_features = torch.tensor(train_features, dtype=torch.float)\n",
    "    train_labels = torch.tensor(train_labels, dtype=torch.float)\n",
    "    test_features = torch.tensor(test_features, dtype=torch.float)\n",
    "    test_labels = torch.tensor(test_labels, dtype=torch.float)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_features, test_labels) \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_pretrain_data(batch_size = 64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop:\n",
    "def train_linear_encoder(model, dataloader, epochs):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(epochs):\n",
    "        for epoch, (X,y) in dataloader:\n",
    "            y_pred = model(X)\n",
    "            loss = nn.MSELoss(y, y_pred) \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearEncoder = RM.LinearAutoencoder(784, 128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
