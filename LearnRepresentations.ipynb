{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yr2z3lARjjM"
      },
      "source": [
        "# Learnig the representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UPG7Xz1GRjjO"
      },
      "outputs": [],
      "source": [
        "# Dependecies\n",
        "import importlib\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# device agnostic code\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "vq2R7D5eSKJQ",
        "outputId": "e792c14c-58fd-4381-ebc4-7a8eda07a387",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzqtBfzbRjjP"
      },
      "source": [
        "## pasted models\n",
        "here for convenience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VkWBUnDgRjjQ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "    Representation learning:\n",
        "    Goal: use unsupervised learning techniques to learn a representation of given data.\n",
        "    The hope is that this representation will be useful to reduce the amount of data that is needed for training the supervised model for solving the actual task.\n",
        "    To verify how good the learned representation is, train a supervised model using these representations that predicts the available pretrain labels.\n",
        "\n",
        "    Methology:\n",
        "    1. Create a several neural networks that learn to encode the data into a representation.\n",
        "    2. Train a supervised model on each of the learned representations. The superverised model trained on the different representations should be very shallow (1 or two fully connected layers) and should be trained for a very short time. The goal is to make the performance of the encoders comparable.\n",
        "'''\n",
        "\n",
        "# Dependencies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "'''\n",
        "    Autoencoder for dimensionality reduction:\n",
        "    Both encoder and decoder using three linear layers\n",
        "'''\n",
        "\n",
        "# for this to make sense the encoding dimension should be significantly smaller than the input dimension\n",
        "# specifically, the encoding_dim*3 shold be smaller than the input_size\n",
        "class LinearAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size, encoding_dim):\n",
        "        super(LinearAutoencoder, self).__init__()\n",
        "        # encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, encoding_dim*3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(int(encoding_dim*3), int(encoding_dim*2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(encoding_dim*2, encoding_dim)\n",
        "        )\n",
        "        # decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, encoding_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(encoding_dim*2, encoding_dim*3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(encoding_dim*3, input_size),\n",
        "            nn.Sigmoid() # the feature values are between 0 and 1\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "'''\n",
        "    Autocoder for dimensionality reduction:\n",
        "    Using three convolutional/deconvolutional layers for encoder/decoder\n",
        "'''\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self, number_filters):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        # encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(1, number_filters*3, kernel_size=3, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(number_filters*3, number_filters*2, kernel_size=3, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(number_filters*2, number_filters, kernel_size=3, stride=2, padding=0)\n",
        "        )\n",
        "        # decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose1d(number_filters, number_filters*2, kernel_size=3, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(number_filters*2, number_filters*3, kernel_size=3, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(number_filters*3, 1, kernel_size=3, stride=2, padding=0, output_padding=1), # need out padding to get the right size\n",
        "            nn.Sigmoid() # the feature values are between 0 and 1\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        x = x.squeeze(1)\n",
        "        return x\n",
        "    \n",
        "'''\n",
        "    Autocoder for dimensionality reduction:\n",
        "    Using two convolutional/deconvolutional layers and one fully connected layer for both encoder and decoder\n",
        "'''\n",
        "class ConvLinearAutoencoder(nn.Module):\n",
        "    def __init__(self, number_filters, encoding_dim):\n",
        "        super(ConvLinearAutoencoder, self).__init__()\n",
        "        self.number_filters = number_filters\n",
        "        # encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(1, number_filters*2, kernel_size=3, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(number_filters*2, number_filters, kernel_size=3, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "        ) \n",
        "        # bottleneck layer\n",
        "        self.fcencoder = nn.Linear(249*number_filters, encoding_dim)\n",
        "        self.fcdecoder = nn.Linear(encoding_dim, 249*number_filters)\n",
        "\n",
        "        # decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose1d(number_filters, number_filters*2, kernel_size=3, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(number_filters*2, 1, kernel_size=3, stride=2, padding=0, output_padding=1),\n",
        "            nn.Sigmoid() # the feature values are between 0 and 1\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.encoder(x)\n",
        "        x = x.view(-1, 249*self.number_filters)\n",
        "        x = self.fcencoder(x)\n",
        "        x = self.fcdecoder(x)\n",
        "        x = x.view(-1,self.number_filters, 249)\n",
        "        x = self.decoder(x)\n",
        "        x = x.squeeze(1)\n",
        "        return x\n",
        "    \n",
        "# contractive loss function\n",
        "def contractive_loss(W, x, recons_x, h, lam=1e-4):\n",
        "    mse_loss = nn.MSELoss()(recons_x, x)\n",
        "    \n",
        "    dh = h * (1 - h) # Derivative of sigmoid\n",
        "    w_sum = torch.sum(Variable(W)**2, dim=1)\n",
        "    w_sum = w_sum.unsqueeze(1) # Shape to 2D tensor\n",
        "    contractive_loss = torch.sum(torch.mm(dh**2, w_sum), 0)\n",
        "    return mse_loss + contractive_loss.mul_(lam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXNY3sIrRjjR"
      },
      "source": [
        "## load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_jSmwLndRjjR",
        "outputId": "de324c8b-bd9f-4540-ca02-c82bf88feed4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Zo1gD5sORjjR"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.unpack_archive(\"drive/MyDrive/SolarEnergyMaterials/task4.zip\", \"/content/data\")\n",
        "shutil.unpack_archive(\"data/task4_hr35z9/pretrain_features.csv.zip\", \"/content/data\")\n",
        "shutil.unpack_archive(\"data/task4_hr35z9/pretrain_labels.csv.zip\", \"/content/data\")\n",
        "shutil.unpack_archive(\"data/task4_hr35z9/train_features.csv.zip\", \"/content/data\")\n",
        "shutil.unpack_archive(\"data/task4_hr35z9/train_labels.csv.zip\", \"/content/data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LoGE3TyCRjjR"
      },
      "outputs": [],
      "source": [
        "def load_pretrain_data(batch_size = 64):\n",
        "    batch_size = 64\n",
        "\n",
        "    random.seed(17)\n",
        "    test_ind = set()\n",
        "\n",
        "    pre_train_size = 50000\n",
        "\n",
        "    while len(test_ind) < 10000: \n",
        "        test_ind.add(random.randint(0, pre_train_size-1))\n",
        "\n",
        "    features =[]\n",
        "    labels = []\n",
        "\n",
        "    with open(\"data/pretrain_features.csv\", 'r') as f:\n",
        "        for row in f:\n",
        "            features.append(row)\n",
        "\n",
        "    with open(\"data/pretrain_labels.csv\", 'r') as f:\n",
        "        for row in f:\n",
        "            labels.append(row)\n",
        "\n",
        "    # remove header\n",
        "    features = features[1:]\n",
        "    labels = labels[1:]\n",
        "\n",
        "    # first try to note use representation of the molecules, only the extracted features\n",
        "    features = [list(map(float,row.split(',')[2:])) for row in features]\n",
        "    labels = [float(row.split(',')[1]) for row in labels]\n",
        "\n",
        "    train_features = []\n",
        "    train_labels = []\n",
        "    test_features = []\n",
        "    test_labels = []\n",
        "\n",
        "\n",
        "    for i in range(len(features)):\n",
        "        if i in test_ind:\n",
        "            test_features.append(features[i])\n",
        "            test_labels.append(labels[i])\n",
        "        else:\n",
        "            train_features.append(features[i])\n",
        "            train_labels.append(labels[i])\n",
        "\n",
        "    # does not seem to make sense to normalize the data since it is very sparse\n",
        "    # normalize train_features\n",
        "    # train_features = (train_features - np.mean(train_features, axis=0)) / (np.std(train_features, axis=0)+EPSILON)\n",
        "\n",
        "    # normalize test_features\n",
        "    # test_features = (test_features - np.mean(test_features, axis=0)) / (np.std(test_features, axis=0)+EPSILON)\n",
        "\n",
        "    # convert into tensor dataset\n",
        "    train_features = torch.tensor(train_features, dtype=torch.float)\n",
        "    train_labels = torch.tensor(train_labels, dtype=torch.float)\n",
        "    test_features = torch.tensor(test_features, dtype=torch.float)\n",
        "    test_labels = torch.tensor(test_labels, dtype=torch.float)\n",
        "\n",
        "    train_dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
        "    test_dataset = torch.utils.data.TensorDataset(test_features, test_labels) \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wxyn3YApRjjS"
      },
      "outputs": [],
      "source": [
        "train_loader, test_loader = load_pretrain_data(batch_size = 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaUjWNeBRjjS"
      },
      "source": [
        "## Train/Test loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dmVh0URARjjS"
      },
      "outputs": [],
      "source": [
        "# train loop:\n",
        "def train_encoder(model, dataloader, epochs):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss=0\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            X_pred = model(X)\n",
        "            loss = loss_fn(X_pred, X)\n",
        "            epoch_loss+=loss.item()\n",
        "            loss.backward()  \n",
        "            optimizer.step()\n",
        "\n",
        "        print('average loss per batch in epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, epoch_loss/len(dataloader)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VqiI15sRjjT"
      },
      "source": [
        "## Linear Autoencoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ounfCfUYRjjT",
        "outputId": "cfb7b27f-0145-42d2-94e2-7c08e6250749",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average loss per batch in epoch [1/10], Loss: 0.0399\n",
            "average loss per batch in epoch [2/10], Loss: 0.0225\n",
            "average loss per batch in epoch [3/10], Loss: 0.0155\n",
            "average loss per batch in epoch [4/10], Loss: 0.0122\n",
            "average loss per batch in epoch [5/10], Loss: 0.0099\n",
            "average loss per batch in epoch [6/10], Loss: 0.0084\n",
            "average loss per batch in epoch [7/10], Loss: 0.0074\n",
            "average loss per batch in epoch [8/10], Loss: 0.0067\n",
            "average loss per batch in epoch [9/10], Loss: 0.0061\n",
            "average loss per batch in epoch [10/10], Loss: 0.0057\n"
          ]
        }
      ],
      "source": [
        "LinearEncoder = LinearAutoencoder(1000, 128)\n",
        "train_encoder(LinearEncoder, train_loader, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UJsTz3DwljMA"
      },
      "outputs": [],
      "source": [
        "torch.save(LinearEncoder.state_dict(), 'drive/MyDrive/SolarEnergyMaterials/PretrainedModels/LinearEncoder.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb6xmnihRjjT"
      },
      "source": [
        "## Convolutional Autoencoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "70xVBFtTRjjT",
        "outputId": "6685b27f-7574-4be2-c6b2-06ff0eb26542",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average loss per batch in epoch [1/10], Loss: 0.0712\n",
            "average loss per batch in epoch [2/10], Loss: 0.0305\n",
            "average loss per batch in epoch [3/10], Loss: 0.0091\n",
            "average loss per batch in epoch [4/10], Loss: 0.0030\n",
            "average loss per batch in epoch [5/10], Loss: 0.0023\n",
            "average loss per batch in epoch [6/10], Loss: 0.0021\n",
            "average loss per batch in epoch [7/10], Loss: 0.0020\n",
            "average loss per batch in epoch [8/10], Loss: 0.0018\n",
            "average loss per batch in epoch [9/10], Loss: 0.0017\n",
            "average loss per batch in epoch [10/10], Loss: 0.0015\n"
          ]
        }
      ],
      "source": [
        "ConvEncoder = ConvAutoencoder(4)\n",
        "test = next(iter(train_loader))\n",
        "train_encoder(ConvEncoder, train_loader, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yKpGkNJ6uifk"
      },
      "outputs": [],
      "source": [
        "torch.save(ConvEncoder.state_dict(), 'drive/MyDrive/SolarEnergyMaterials/PretrainedModels/ConvEncoder.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuXPuMCQRjjU"
      },
      "source": [
        "## Convolutional Autoencoder with Linear Layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4DTjqLGERjjU",
        "outputId": "0404d060-72b4-46d0-c146-e88589c590df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average loss per batch in epoch [1/10], Loss: 0.0248\n",
            "average loss per batch in epoch [2/10], Loss: 0.0081\n",
            "average loss per batch in epoch [3/10], Loss: 0.0068\n",
            "average loss per batch in epoch [4/10], Loss: 0.0063\n",
            "average loss per batch in epoch [5/10], Loss: 0.0060\n",
            "average loss per batch in epoch [6/10], Loss: 0.0058\n",
            "average loss per batch in epoch [7/10], Loss: 0.0057\n",
            "average loss per batch in epoch [8/10], Loss: 0.0056\n",
            "average loss per batch in epoch [9/10], Loss: 0.0055\n",
            "average loss per batch in epoch [10/10], Loss: 0.0054\n"
          ]
        }
      ],
      "source": [
        "ConvLinearEncoder = ConvLinearAutoencoder(6, 90)\n",
        "train_encoder(ConvLinearEncoder, train_loader, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4zjeG7RZwgce"
      },
      "outputs": [],
      "source": [
        "torch.save(ConvEncoder.state_dict(), 'drive/MyDrive/SolarEnergyMaterials/PretrainedModels/ConvEncoder.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the representations\n",
        "With the pretrain data"
      ],
      "metadata": {
        "id": "2JR-TNn7vsqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_weights(model):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n"
      ],
      "metadata": {
        "id": "BdO1gVc3v0Ec"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BSrWQ8pfyATY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}