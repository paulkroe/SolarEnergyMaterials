{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulkroe/SolarEnergyMaterials/blob/main/LearnRepresentations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yr2z3lARjjM"
      },
      "source": [
        "# Learnig the representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UPG7Xz1GRjjO"
      },
      "outputs": [],
      "source": [
        "# Dependecies\n",
        "import importlib\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# device agnostic code\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "vq2R7D5eSKJQ",
        "outputId": "dc6d33c1-5bf5-43e1-f2b7-1b9f0308c805",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzqtBfzbRjjP"
      },
      "source": [
        "## pasted models\n",
        "here for convenience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VkWBUnDgRjjQ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "    Representation learning:\n",
        "    Goal: use unsupervised learning techniques to learn a representation of given data.\n",
        "    The hope is that this representation will be useful to reduce the amount of data that is needed for training the supervised model for solving the actual task.\n",
        "    To verify how good the learned representation is, train a supervised model using these representations that predicts the available pretrain labels.\n",
        "\n",
        "    Methology:\n",
        "    1. Create a several neural networks that learn to encode the data into a representation.\n",
        "    2. Train a supervised model on each of the learned representations. The superverised model trained on the different representations should be very shallow (1 or two fully connected layers) and should be trained for a very short time. The goal is to make the performance of the encoders comparable.\n",
        "'''\n",
        "\n",
        "# Dependencies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "'''\n",
        "    Autoencoder for dimensionality reduction:\n",
        "    Both encoder and decoder using three linear layers\n",
        "'''\n",
        "\n",
        "# for this to make sense the encoding dimension should be significantly smaller than the input dimension\n",
        "# specifically, the encoding_dim*3 shold be smaller than the input_size\n",
        "class LinearAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size, encoding_dim):\n",
        "        super(LinearAutoencoder, self).__init__()\n",
        "        # encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, encoding_dim*3),\n",
        "            nn.BatchNorm1d(encoding_dim*3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(int(encoding_dim*3), int(encoding_dim*2)),\n",
        "            nn.BatchNorm1d(int(encoding_dim*2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(encoding_dim*2, encoding_dim),\n",
        "        )\n",
        "        # decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, encoding_dim*2),\n",
        "            nn.BatchNorm1d(encoding_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(encoding_dim*2, encoding_dim*3),\n",
        "            nn.BatchNorm1d(encoding_dim*3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(encoding_dim*3, input_size),\n",
        "            nn.Sigmoid() # the feature values are between 0 and 1\n",
        "        )\n",
        "\n",
        "        # fully connected layer for pretrain task\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, pretrain = False):\n",
        "        if pretrain:\n",
        "            x = self.encoder(x)\n",
        "            x = self.fc(x)\n",
        "            x = x.squeeze(1)\n",
        "        else:\n",
        "            x = self.encoder(x)\n",
        "            x = self.decoder(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self, number_filters):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        self.number_filters = number_filters\n",
        "        # encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(1, number_filters*3, kernel_size=3, stride=2, padding=0),\n",
        "            nn.BatchNorm1d(number_filters*3),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(number_filters*3, number_filters*2, kernel_size=3, stride=2, padding=0),\n",
        "            nn.BatchNorm1d(number_filters*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(number_filters*2, number_filters, kernel_size=3, stride=2, padding=0),\n",
        "            nn.BatchNorm1d(number_filters)\n",
        "        )\n",
        "        # decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose1d(number_filters, number_filters*2, kernel_size=3, stride=2, padding=0),\n",
        "            nn.BatchNorm1d(number_filters*2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(number_filters*2, number_filters*3, kernel_size=3, stride=2, padding=0),\n",
        "            nn.BatchNorm1d(number_filters*3),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(number_filters*3, 1, kernel_size=3, stride=2, padding=0, output_padding=1), \n",
        "            nn.Sigmoid() # the feature values are between 0 and 1\n",
        "        )\n",
        "        # fully connected layer for pretrain task\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(124*number_filters, 1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x, pretrain=False):\n",
        "        if pretrain:\n",
        "          x = x.unsqueeze(1)\n",
        "          x = self.encoder(x)\n",
        "          x = x.view(-1, 124*self.number_filters)     \n",
        "          x = self.fc(x)\n",
        "          x = x.squeeze(1)\n",
        "        else:\n",
        "          x = x.unsqueeze(1)\n",
        "          x = self.encoder(x)\n",
        "          x = self.decoder(x)\n",
        "          x = x.squeeze(1)\n",
        "        return x\n",
        "       "
      ],
      "metadata": {
        "id": "3Tph9-bR5Guq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "    Autocoder for dimensionality reduction:\n",
        "    Using two convolutional/deconvolutional layers and one fully connected layer for both encoder and decoder\n",
        "'''\n",
        "class ConvLinearAutoencoder(nn.Module):\n",
        "    def __init__(self, number_filters, encoding_dim):\n",
        "        super(ConvLinearAutoencoder, self).__init__()\n",
        "        self.number_filters = number_filters\n",
        "        # encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(1, number_filters*2, kernel_size=3, stride=2, padding=0),\n",
        "            nn.BatchNorm1d(number_filters*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(number_filters*2, number_filters, kernel_size=3, stride=2, padding=0),\n",
        "            nn.BatchNorm1d(number_filters),\n",
        "            nn.ReLU(),\n",
        "        ) \n",
        "        # bottleneck layer\n",
        "        self.fcencoder = nn.Sequential(\n",
        "            nn.Linear(249*number_filters, encoding_dim),\n",
        "            nn.BatchNorm1d(encoding_dim),\n",
        "        )\n",
        "        self.fcdecoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, 249*number_filters),\n",
        "            nn.BatchNorm1d(249*number_filters),\n",
        "        )\n",
        "\n",
        "        # decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose1d(number_filters, number_filters*2, kernel_size=3, stride=2, padding=0),\n",
        "            nn.BatchNorm1d(number_filters*2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(number_filters*2, 1, kernel_size=3, stride=2, padding=0, output_padding=1),\n",
        "            nn.Sigmoid() # the feature values are between 0 and 1\n",
        "        )\n",
        "\n",
        "        # fully connected layer for pretrain task\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, 1),\n",
        "            nn.BatchNorm1d(1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, pretrain = False):\n",
        "        if pretrain:\n",
        "            x = x.unsqueeze(1)\n",
        "            x = self.encoder(x)\n",
        "            x = x.view(-1, 249*self.number_filters)\n",
        "            x = self.fcencoder(x)\n",
        "            x = self.fc(x)\n",
        "            x = x.squeeze(1)\n",
        "        else:\n",
        "            x = x.unsqueeze(1)\n",
        "            x = self.encoder(x)\n",
        "            x = x.view(-1, 249*self.number_filters)\n",
        "            x = self.fcencoder(x)\n",
        "            x = self.fcdecoder(x)\n",
        "            x = x.view(-1,self.number_filters, 249)\n",
        "            x = self.decoder(x)\n",
        "            x = x.squeeze(1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "uPnfSj-U5HDN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# contractive loss function\n",
        "def contractive_loss(W, x, recons_x, h, lam=1e-4):\n",
        "    mse_loss = nn.MSELoss()(recons_x, x)\n",
        "    \n",
        "    dh = h * (1 - h) # Derivative of sigmoid\n",
        "    w_sum = torch.sum(Variable(W)**2, dim=1)\n",
        "    w_sum = w_sum.unsqueeze(1) # Shape to 2D tensor\n",
        "    contractive_loss = torch.sum(torch.mm(dh**2, w_sum), 0)\n",
        "    return mse_loss + contractive_loss.mul_(lam)"
      ],
      "metadata": {
        "id": "XLsFoVrZ5I6s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXNY3sIrRjjR"
      },
      "source": [
        "## load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_jSmwLndRjjR",
        "outputId": "7a673fcf-bc42-4d6e-cd51-efb4ac374b6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Zo1gD5sORjjR"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.unpack_archive(\"drive/MyDrive/SolarEnergyMaterials/task4.zip\", \"/content/data\")\n",
        "shutil.unpack_archive(\"data/task4_hr35z9/pretrain_features.csv.zip\", \"/content/data\")\n",
        "shutil.unpack_archive(\"data/task4_hr35z9/pretrain_labels.csv.zip\", \"/content/data\")\n",
        "shutil.unpack_archive(\"data/task4_hr35z9/train_features.csv.zip\", \"/content/data\")\n",
        "shutil.unpack_archive(\"data/task4_hr35z9/train_labels.csv.zip\", \"/content/data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LoGE3TyCRjjR"
      },
      "outputs": [],
      "source": [
        "def load_pretrain_data(batch_size = 64):\n",
        "    batch_size = 64\n",
        "\n",
        "    random.seed(17)\n",
        "    test_ind = set()\n",
        "\n",
        "    pre_train_size = 50000\n",
        "\n",
        "    while len(test_ind) < 10000: \n",
        "        test_ind.add(random.randint(0, pre_train_size-1))\n",
        "\n",
        "    features =[]\n",
        "    labels = []\n",
        "\n",
        "    with open(\"data/pretrain_features.csv\", 'r') as f:\n",
        "        for row in f:\n",
        "            features.append(row)\n",
        "\n",
        "    with open(\"data/pretrain_labels.csv\", 'r') as f:\n",
        "        for row in f:\n",
        "            labels.append(row)\n",
        "\n",
        "    # remove header\n",
        "    features = features[1:]\n",
        "    labels = labels[1:]\n",
        "\n",
        "    # first try to note use representation of the molecules, only the extracted features\n",
        "    features = [list(map(float,row.split(',')[2:])) for row in features]\n",
        "    labels = [float(row.split(',')[1]) for row in labels]\n",
        "\n",
        "    train_features = []\n",
        "    train_labels = []\n",
        "    test_features = []\n",
        "    test_labels = []\n",
        "\n",
        "\n",
        "    for i in range(len(features)):\n",
        "        if i in test_ind:\n",
        "            test_features.append(features[i])\n",
        "            test_labels.append(labels[i])\n",
        "        else:\n",
        "            train_features.append(features[i])\n",
        "            train_labels.append(labels[i])\n",
        "\n",
        "    # does not seem to make sense to normalize the data since it is very sparse\n",
        "    # normalize train_features\n",
        "    # train_features = (train_features - np.mean(train_features, axis=0)) / (np.std(train_features, axis=0)+EPSILON)\n",
        "\n",
        "    # normalize test_features\n",
        "    # test_features = (test_features - np.mean(test_features, axis=0)) / (np.std(test_features, axis=0)+EPSILON)\n",
        "\n",
        "    # convert into tensor dataset\n",
        "    train_features = torch.tensor(train_features, dtype=torch.float)\n",
        "    train_labels = torch.tensor(train_labels, dtype=torch.float)\n",
        "    test_features = torch.tensor(test_features, dtype=torch.float)\n",
        "    test_labels = torch.tensor(test_labels, dtype=torch.float)\n",
        "\n",
        "    train_dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
        "    test_dataset = torch.utils.data.TensorDataset(test_features, test_labels) \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wxyn3YApRjjS"
      },
      "outputs": [],
      "source": [
        "train_loader, test_loader = load_pretrain_data(batch_size = 64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_finetune_data(batch_size = 4):\n",
        "    batch_size = 4\n",
        "\n",
        "    random.seed(17)\n",
        "    test_ind = set()\n",
        "\n",
        "    pre_train_size = 100\n",
        "    while len(test_ind) < 50: \n",
        "        test_ind.add(random.randint(0, pre_train_size-1))\n",
        "\n",
        "    features =[]\n",
        "    labels = []\n",
        "\n",
        "    with open(\"data/train_features.csv\", 'r') as f:\n",
        "        for row in f:\n",
        "            features.append(row)\n",
        "\n",
        "    with open(\"data/train_labels.csv\", 'r') as f:\n",
        "        for row in f:\n",
        "            labels.append(row)\n",
        "\n",
        "    # remove header\n",
        "    features = features[1:]\n",
        "    labels = labels[1:]\n",
        "\n",
        "    # first try to note use representation of the molecules, only the extracted features\n",
        "    features = [list(map(float,row.split(',')[2:])) for row in features]\n",
        "    labels = [float(row.split(',')[1]) for row in labels]\n",
        "\n",
        "    train_features = []\n",
        "    train_labels = []\n",
        "    test_features = []\n",
        "    test_labels = []\n",
        "\n",
        "\n",
        "    for i in range(len(features)):\n",
        "        if i in test_ind:\n",
        "            test_features.append(features[i])\n",
        "            test_labels.append(labels[i])\n",
        "        else:\n",
        "            train_features.append(features[i])\n",
        "            train_labels.append(labels[i])\n",
        "\n",
        "    # does not seem to make sense to normalize the data since it is very sparse\n",
        "    # normalize train_features\n",
        "    # train_features = (train_features - np.mean(train_features, axis=0)) / (np.std(train_features, axis=0)+EPSILON)\n",
        "\n",
        "    # normalize test_features\n",
        "    # test_features = (test_features - np.mean(test_features, axis=0)) / (np.std(test_features, axis=0)+EPSILON)\n",
        "\n",
        "    # convert into tensor dataset\n",
        "    train_features = torch.tensor(train_features, dtype=torch.float)\n",
        "    train_labels = torch.tensor(train_labels, dtype=torch.float)\n",
        "    test_features = torch.tensor(test_features, dtype=torch.float)\n",
        "    test_labels = torch.tensor(test_labels, dtype=torch.float)\n",
        "\n",
        "    train_dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
        "    test_dataset = torch.utils.data.TensorDataset(test_features, test_labels) \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "uJIKQJZzE8Mm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_train_loader, finetune_test_loader = load_finetune_data(batch_size=4)"
      ],
      "metadata": {
        "id": "J3Tqhug4FQtI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaUjWNeBRjjS"
      },
      "source": [
        "## Train/Test loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dmVh0URARjjS"
      },
      "outputs": [],
      "source": [
        "# train loop:\n",
        "def train_encoder(model, dataloader, epochs, pretrain=False):\n",
        "    if pretrain:\n",
        "      freeze_weights(model.encoder)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss=0\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "          if pretrain:\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(X, pretrain=True)\n",
        "            loss = loss_fn(y, y_pred)\n",
        "            epoch_loss+=loss.item()\n",
        "            loss.backward()  \n",
        "            optimizer.step()\n",
        "          else:\n",
        "            optimizer.zero_grad()\n",
        "            X_pred = model(X)\n",
        "            loss = loss_fn(X_pred, X)\n",
        "            epoch_loss+=loss.item()\n",
        "            loss.backward()  \n",
        "            optimizer.step()\n",
        "\n",
        "        print('average loss per batch in epoch [{}/{}], Loss: {:.6f}'.format(epoch+1, epochs, epoch_loss/len(dataloader)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test loop\n",
        "def test_model(model, data_loader):\n",
        "  loss_fn = nn.MSELoss() \n",
        "  model.to(device)\n",
        "  Y = torch.tensor([]).to(device)\n",
        "  Y_pred = torch.tensor([]).to(device)\n",
        "  with torch.no_grad():\n",
        "    for batch, (X,y) in enumerate(data_loader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      y_pred = model(X, pretrain=True)\n",
        "      Y = torch.cat((Y, y))\n",
        "      Y_pred = torch.cat((Y_pred, y_pred))\n",
        "    loss = torch.sqrt(loss_fn(y_pred, y))\n",
        "    print(f\"average batch loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "j7vw_Yyk1EKb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_weights(model):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n"
      ],
      "metadata": {
        "id": "BdO1gVc3v0Ec"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VqiI15sRjjT"
      },
      "source": [
        "## Linear Autoencoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ounfCfUYRjjT",
        "outputId": "830e02b1-f676-43c2-cc69-6d0a2a5f5c8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average loss per batch in epoch [1/10], Loss: 0.017232\n",
            "average loss per batch in epoch [2/10], Loss: 0.007259\n",
            "average loss per batch in epoch [3/10], Loss: 0.005316\n",
            "average loss per batch in epoch [4/10], Loss: 0.004196\n",
            "average loss per batch in epoch [5/10], Loss: 0.003411\n",
            "average loss per batch in epoch [6/10], Loss: 0.002907\n",
            "average loss per batch in epoch [7/10], Loss: 0.002534\n",
            "average loss per batch in epoch [8/10], Loss: 0.002269\n",
            "average loss per batch in epoch [9/10], Loss: 0.002070\n",
            "average loss per batch in epoch [10/10], Loss: 0.001909\n"
          ]
        }
      ],
      "source": [
        "LinearEncoder = LinearAutoencoder(1000, 128)\n",
        "train_encoder(LinearEncoder, train_loader, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UJsTz3DwljMA"
      },
      "outputs": [],
      "source": [
        "torch.save(LinearEncoder.state_dict(), 'drive/MyDrive/SolarEnergyMaterials/PretrainedModels/LinearEncoder.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb6xmnihRjjT"
      },
      "source": [
        "## Convolutional Autoencoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "70xVBFtTRjjT",
        "outputId": "4bd06382-da21-4744-b89b-9930e0e03842",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average loss per batch in epoch [1/10], Loss: 0.033020\n",
            "average loss per batch in epoch [2/10], Loss: 0.006542\n",
            "average loss per batch in epoch [3/10], Loss: 0.001901\n",
            "average loss per batch in epoch [4/10], Loss: 0.000984\n",
            "average loss per batch in epoch [5/10], Loss: 0.000596\n",
            "average loss per batch in epoch [6/10], Loss: 0.000337\n",
            "average loss per batch in epoch [7/10], Loss: 0.000266\n",
            "average loss per batch in epoch [8/10], Loss: 0.000220\n",
            "average loss per batch in epoch [9/10], Loss: 0.000199\n",
            "average loss per batch in epoch [10/10], Loss: 0.000183\n"
          ]
        }
      ],
      "source": [
        "ConvEncoder = ConvAutoencoder(4)\n",
        "test = next(iter(train_loader))\n",
        "train_encoder(ConvEncoder, train_loader, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yKpGkNJ6uifk"
      },
      "outputs": [],
      "source": [
        "torch.save(ConvEncoder.state_dict(), 'drive/MyDrive/SolarEnergyMaterials/PretrainedModels/ConvEncoder.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuXPuMCQRjjU"
      },
      "source": [
        "## Convolutional Autoencoder with Linear Layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4DTjqLGERjjU",
        "outputId": "59dbf27b-21ce-4e0a-cfbb-0be9fc6ca135",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average loss per batch in epoch [1/10], Loss: 0.066300\n",
            "average loss per batch in epoch [2/10], Loss: 0.008494\n",
            "average loss per batch in epoch [3/10], Loss: 0.004696\n",
            "average loss per batch in epoch [4/10], Loss: 0.003250\n",
            "average loss per batch in epoch [5/10], Loss: 0.002525\n",
            "average loss per batch in epoch [6/10], Loss: 0.002059\n",
            "average loss per batch in epoch [7/10], Loss: 0.001743\n",
            "average loss per batch in epoch [8/10], Loss: 0.001523\n",
            "average loss per batch in epoch [9/10], Loss: 0.001357\n",
            "average loss per batch in epoch [10/10], Loss: 0.001229\n"
          ]
        }
      ],
      "source": [
        "ConvLinearEncoder = ConvLinearAutoencoder(6, 90) # almost no compression if product close to 1000\n",
        "train_encoder(ConvLinearEncoder, train_loader, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4zjeG7RZwgce"
      },
      "outputs": [],
      "source": [
        "torch.save(ConvEncoder.state_dict(), 'drive/MyDrive/SolarEnergyMaterials/PretrainedModels/ConvEncoder.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the representations\n",
        "With the pretrain data"
      ],
      "metadata": {
        "id": "2JR-TNn7vsqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Model\n"
      ],
      "metadata": {
        "id": "6zoYFVzK7ayN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoder(LinearEncoder, train_loader, 10, pretrain=True)\n",
        "test_model(LinearEncoder, train_loader)\n",
        "print('---')\n",
        "test_model(LinearEncoder, test_loader)"
      ],
      "metadata": {
        "id": "BSrWQ8pfyATY",
        "outputId": "2cce3320-bd43-4188-88d1-e8b5e18fe32c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average loss per batch in epoch [1/10], Loss: 0.461614\n",
            "average loss per batch in epoch [2/10], Loss: 0.087172\n",
            "average loss per batch in epoch [3/10], Loss: 0.075728\n",
            "average loss per batch in epoch [4/10], Loss: 0.067485\n",
            "average loss per batch in epoch [5/10], Loss: 0.061753\n",
            "average loss per batch in epoch [6/10], Loss: 0.056698\n",
            "average loss per batch in epoch [7/10], Loss: 0.054565\n",
            "average loss per batch in epoch [8/10], Loss: 0.052156\n",
            "average loss per batch in epoch [9/10], Loss: 0.050331\n",
            "average loss per batch in epoch [10/10], Loss: 0.049903\n",
            "average batch loss: 0.2667633295059204\n",
            "---\n",
            "average batch loss: 0.1762481927871704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conv Model\n"
      ],
      "metadata": {
        "id": "5LWz-RLu7gnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoder(ConvEncoder, train_loader, 10, pretrain=True)\n",
        "test_model(ConvEncoder, train_loader)\n",
        "print('---')\n",
        "test_model(ConvEncoder, test_loader)"
      ],
      "metadata": {
        "id": "7yYbkWlU7fdD",
        "outputId": "49226d53-9ba6-4c18-c48e-be1c1d42b745",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average loss per batch in epoch [1/10], Loss: 0.333644\n",
            "average loss per batch in epoch [2/10], Loss: 0.071668\n",
            "average loss per batch in epoch [3/10], Loss: 0.055855\n",
            "average loss per batch in epoch [4/10], Loss: 0.050241\n",
            "average loss per batch in epoch [5/10], Loss: 0.047170\n",
            "average loss per batch in epoch [6/10], Loss: 0.045823\n",
            "average loss per batch in epoch [7/10], Loss: 0.043739\n",
            "average loss per batch in epoch [8/10], Loss: 0.042354\n",
            "average loss per batch in epoch [9/10], Loss: 0.041588\n",
            "average loss per batch in epoch [10/10], Loss: 0.039851\n",
            "average batch loss: 0.1723169982433319\n",
            "---\n",
            "average batch loss: 0.14731952548027039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conv Linear Model"
      ],
      "metadata": {
        "id": "wRcDqj5m7jxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoder(ConvLinearEncoder, train_loader, 10, pretrain=True) # why is the initial loss so high compared to the others, might suggest that the representations are not working?\n",
        "test_model(ConvLinearEncoder, train_loader)\n",
        "print('---')\n",
        "test_model(ConvLinearEncoder, test_loader) "
      ],
      "metadata": {
        "id": "69yiakoh7pZp",
        "outputId": "53df1c98-79b8-4b62-8442-8d3340e54784",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average loss per batch in epoch [1/10], Loss: 9.193672\n",
            "average loss per batch in epoch [2/10], Loss: 5.929078\n",
            "average loss per batch in epoch [3/10], Loss: 3.673273\n",
            "average loss per batch in epoch [4/10], Loss: 2.110967\n",
            "average loss per batch in epoch [5/10], Loss: 1.092343\n",
            "average loss per batch in epoch [6/10], Loss: 0.490752\n",
            "average loss per batch in epoch [7/10], Loss: 0.185499\n",
            "average loss per batch in epoch [8/10], Loss: 0.061543\n",
            "average loss per batch in epoch [9/10], Loss: 0.025520\n",
            "average loss per batch in epoch [10/10], Loss: 0.018551\n",
            "average batch loss: 0.13092286884784698\n",
            "---\n",
            "average batch loss: 0.10595816373825073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model on task\n",
        "Goal is to test the hypothesis that good representations make it easier to train models on a wide range of differnent tasks. More precisely:\n",
        "If a model create encodings that lead to relatively good performance on the pretraining problem, then we expect it to score similar (relatively) on the actual problem. For starters we will just use one linear layer to train ontop of the encoders.\n"
      ],
      "metadata": {
        "id": "TYYfv-EuD-1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Linear Encoder"
      ],
      "metadata": {
        "id": "a_KanfbUJIy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using not pretrained model\n",
        "# using control to check whether it makes a diffrence to learn the representations at all\n",
        "controlLinearEncoder = LinearAutoencoder(1000, 128)\n",
        "expLinearEncoder = LinearAutoencoder(1000, 128)\n",
        "train_encoder(expLinearEncoder, train_loader, 10)\n",
        "torch.save(expLinearEncoder.state_dict(), 'drive/MyDrive/SolarEnergyMaterials/PretrainedModels/expLinearEncoder.pth')\n",
        "torch.save(controlLinearEncoder.state_dict(), 'drive/MyDrive/SolarEnergyMaterials/PretrainedModels/controlLinearEncoder.pth')"
      ],
      "metadata": {
        "id": "GOq_sCX-Eotu",
        "outputId": "26d585df-bb25-481e-9584-cbb0496b9048",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average loss per batch in epoch [1/10], Loss: 0.017215\n",
            "average loss per batch in epoch [2/10], Loss: 0.007233\n",
            "average loss per batch in epoch [3/10], Loss: 0.005292\n",
            "average loss per batch in epoch [4/10], Loss: 0.004201\n",
            "average loss per batch in epoch [5/10], Loss: 0.003455\n",
            "average loss per batch in epoch [6/10], Loss: 0.002937\n",
            "average loss per batch in epoch [7/10], Loss: 0.002558\n",
            "average loss per batch in epoch [8/10], Loss: 0.002277\n",
            "average loss per batch in epoch [9/10], Loss: 0.002084\n",
            "average loss per batch in epoch [10/10], Loss: 0.001910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train on actual problem\n",
        "controlLinearEncoder = LinearAutoencoder(1000, 128)\n",
        "expLinearEncoder = LinearAutoencoder(1000, 128)\n",
        "controlLinearEncoder.load_state_dict(torch.load('drive/MyDrive/SolarEnergyMaterials/PretrainedModels/controlLinearEncoder.pth'),strict=True)\n",
        "expLinearEncoder.load_state_dict(torch.load('drive/MyDrive/SolarEnergyMaterials/PretrainedModels/expLinearEncoder.pth'),strict=True)\n",
        "\n",
        "train_encoder(expLinearEncoder, finetune_train_loader, epochs = 10, pretrain = True)\n",
        "print('---')\n",
        "train_encoder(controlLinearEncoder, finetune_train_loader, epochs=10, pretrain = True)\n",
        "print('------')\n",
        "test_model(expLinearEncoder, finetune_train_loader)\n",
        "test_model(expLinearEncoder, finetune_test_loader)\n",
        "print('---')\n",
        "test_model(controlLinearEncoder, finetune_train_loader)\n",
        "test_model(controlLinearEncoder, finetune_test_loader)"
      ],
      "metadata": {
        "id": "RJDkOlILHX7l",
        "outputId": "7102181c-0bb2-4823-b7e7-d5cbbee3be4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average loss per batch in epoch [1/10], Loss: 3.878172\n",
            "average loss per batch in epoch [2/10], Loss: 1.265308\n",
            "average loss per batch in epoch [3/10], Loss: 0.945516\n",
            "average loss per batch in epoch [4/10], Loss: 0.620536\n",
            "average loss per batch in epoch [5/10], Loss: 0.561581\n",
            "average loss per batch in epoch [6/10], Loss: 0.436199\n",
            "average loss per batch in epoch [7/10], Loss: 0.592600\n",
            "average loss per batch in epoch [8/10], Loss: 0.530182\n",
            "average loss per batch in epoch [9/10], Loss: 0.236625\n",
            "average loss per batch in epoch [10/10], Loss: 0.208359\n",
            "---\n",
            "average loss per batch in epoch [1/10], Loss: 2.773518\n",
            "average loss per batch in epoch [2/10], Loss: 1.806972\n",
            "average loss per batch in epoch [3/10], Loss: 1.161953\n",
            "average loss per batch in epoch [4/10], Loss: 0.700795\n",
            "average loss per batch in epoch [5/10], Loss: 0.432720\n",
            "average loss per batch in epoch [6/10], Loss: 0.282883\n",
            "average loss per batch in epoch [7/10], Loss: 0.206136\n",
            "average loss per batch in epoch [8/10], Loss: 0.176259\n",
            "average loss per batch in epoch [9/10], Loss: 0.136016\n",
            "average loss per batch in epoch [10/10], Loss: 0.181417\n",
            "------\n",
            "average batch loss: 0.22980521619319916\n",
            "average batch loss: 1.0367194414138794\n",
            "---\n",
            "average batch loss: 0.8526756167411804\n",
            "average batch loss: 0.6669043302536011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convolutioal Encoder"
      ],
      "metadata": {
        "id": "9BWVjUy5JQbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using not pretrained model\n",
        "# using control to check whether it makes a diffrence to learn the representations at all\n",
        "controlConvEncoder = ConvAutoencoder(4)\n",
        "expConvEncoder = ConvAutoencoder(4)\n",
        "train_encoder(expConvEncoder, train_loader, 10)\n",
        "torch.save(expConvEncoder.state_dict(), 'drive/MyDrive/SolarEnergyMaterials/PretrainedModels/expConvEncoder.pth')\n",
        "torch.save(controlConvEncoder.state_dict(), 'drive/MyDrive/SolarEnergyMaterials/PretrainedModels/controlConvEncoder.pth')"
      ],
      "metadata": {
        "id": "RwlRLbjpJWcY",
        "outputId": "f45f2408-6de0-43db-9bb1-deedc289d800",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average loss per batch in epoch [1/10], Loss: 0.071669\n",
            "average loss per batch in epoch [2/10], Loss: 0.003988\n",
            "average loss per batch in epoch [3/10], Loss: 0.001851\n",
            "average loss per batch in epoch [4/10], Loss: 0.001323\n",
            "average loss per batch in epoch [5/10], Loss: 0.001168\n",
            "average loss per batch in epoch [6/10], Loss: 0.001089\n",
            "average loss per batch in epoch [7/10], Loss: 0.000840\n",
            "average loss per batch in epoch [8/10], Loss: 0.000657\n",
            "average loss per batch in epoch [9/10], Loss: 0.000560\n",
            "average loss per batch in epoch [10/10], Loss: 0.000472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train on actual problem\n",
        "controlConvEncoder = ConvAutoencoder(4)\n",
        "expConvEncoder = ConvAutoencoder(4)\n",
        "controlConvEncoder.load_state_dict(torch.load('drive/MyDrive/SolarEnergyMaterials/PretrainedModels/controlConvEncoder.pth'),strict=True)\n",
        "expConvEncoder.load_state_dict(torch.load('drive/MyDrive/SolarEnergyMaterials/PretrainedModels/expConvEncoder.pth'),strict=True)\n",
        "train_encoder(expConvEncoder, finetune_train_loader, epochs = 10, pretrain = True)\n",
        "print('---')\n",
        "train_encoder(controlConvEncoder, finetune_train_loader, epochs=10, pretrain = True)\n",
        "print('------')\n",
        "test_model(expConvEncoder, finetune_train_loader)\n",
        "test_model(expConvEncoder, finetune_test_loader)\n",
        "print('---')\n",
        "test_model(controlConvEncoder, finetune_train_loader)\n",
        "test_model(controlConvEncoder, finetune_test_loader)"
      ],
      "metadata": {
        "id": "AzclkpFBKZ_F",
        "outputId": "cc45c7f0-fae5-4ad6-ad05-a7029956553a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average loss per batch in epoch [1/10], Loss: 1.764649\n",
            "average loss per batch in epoch [2/10], Loss: 0.524801\n",
            "average loss per batch in epoch [3/10], Loss: 0.369193\n",
            "average loss per batch in epoch [4/10], Loss: 0.179534\n",
            "average loss per batch in epoch [5/10], Loss: 0.129048\n",
            "average loss per batch in epoch [6/10], Loss: 0.078349\n",
            "average loss per batch in epoch [7/10], Loss: 0.047434\n",
            "average loss per batch in epoch [8/10], Loss: 0.038616\n",
            "average loss per batch in epoch [9/10], Loss: 0.030425\n",
            "average loss per batch in epoch [10/10], Loss: 0.024886\n",
            "---\n",
            "average loss per batch in epoch [1/10], Loss: 3.798221\n",
            "average loss per batch in epoch [2/10], Loss: 0.387137\n",
            "average loss per batch in epoch [3/10], Loss: 0.294806\n",
            "average loss per batch in epoch [4/10], Loss: 0.173692\n",
            "average loss per batch in epoch [5/10], Loss: 0.118407\n",
            "average loss per batch in epoch [6/10], Loss: 0.093576\n",
            "average loss per batch in epoch [7/10], Loss: 0.072977\n",
            "average loss per batch in epoch [8/10], Loss: 0.043856\n",
            "average loss per batch in epoch [9/10], Loss: 0.041563\n",
            "average loss per batch in epoch [10/10], Loss: 0.034864\n",
            "------\n",
            "average batch loss: 0.02144302986562252\n",
            "average batch loss: 0.5310534238815308\n",
            "---\n",
            "average batch loss: 0.21899084746837616\n",
            "average batch loss: 0.2167952060699463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convolutional Linear Encoder\n"
      ],
      "metadata": {
        "id": "G6ATb7R2J8-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using not pretrained model\n",
        "# using control to check whether it makes a diffrence to learn the representations at all\n",
        "controlConvLinearEncoder = ConvLinearAutoencoder(6,90)\n",
        "expConvLinearEncoder = ConvLinearAutoencoder(6,90)\n",
        "train_encoder(expConvLinearEncoder, train_loader, 10)\n",
        "torch.save(expConvLinearEncoder.state_dict(), 'drive/MyDrive/SolarEnergyMaterials/PretrainedModels/expConvLinearEncoder.pth')\n",
        "torch.save(controlConvLinearEncoder.state_dict(), 'drive/MyDrive/SolarEnergyMaterials/PretrainedModels/controlConvLinearEncoder.pth')"
      ],
      "metadata": {
        "id": "IA6QX2kTKE16",
        "outputId": "e10e2144-d32d-437e-f9d0-724a7cfc34e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average loss per batch in epoch [1/10], Loss: 0.073812\n",
            "average loss per batch in epoch [2/10], Loss: 0.008143\n",
            "average loss per batch in epoch [3/10], Loss: 0.004759\n",
            "average loss per batch in epoch [4/10], Loss: 0.003401\n",
            "average loss per batch in epoch [5/10], Loss: 0.002687\n",
            "average loss per batch in epoch [6/10], Loss: 0.002248\n",
            "average loss per batch in epoch [7/10], Loss: 0.001942\n",
            "average loss per batch in epoch [8/10], Loss: 0.001711\n",
            "average loss per batch in epoch [9/10], Loss: 0.001558\n",
            "average loss per batch in epoch [10/10], Loss: 0.001409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train on actual problem\n",
        "controlConvLinearEncoder = ConvLinearAutoencoder(6,90)\n",
        "expConvLinearEncoder = ConvLinearAutoencoder(6,90)\n",
        "controlConvLinearEncoder.load_state_dict(torch.load('drive/MyDrive/SolarEnergyMaterials/PretrainedModels/controlConvLinearEncoder.pth'),strict=True)\n",
        "expConvLinearEncoder.load_state_dict(torch.load('drive/MyDrive/SolarEnergyMaterials/PretrainedModels/expConvLinearEncoder.pth'),strict=True)\n",
        "train_encoder(expConvLinearEncoder, finetune_train_loader, epochs = 10, pretrain = True)\n",
        "print('---')\n",
        "train_encoder(controlConvLinearEncoder, finetune_train_loader, epochs=10, pretrain = True)\n",
        "print('------')\n",
        "test_model(expConvLinearEncoder, finetune_train_loader)\n",
        "test_model(expConvLinearEncoder, finetune_test_loader)\n",
        "print('---')\n",
        "test_model(controlConvLinearEncoder, finetune_train_loader)\n",
        "test_model(controlConvLinearEncoder, finetune_test_loader)"
      ],
      "metadata": {
        "id": "hvWVzJZsKshH",
        "outputId": "c7e1dc6d-e3e3-4847-fb70-a428832ade71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average loss per batch in epoch [1/10], Loss: 4.249253\n",
            "average loss per batch in epoch [2/10], Loss: 4.008540\n",
            "average loss per batch in epoch [3/10], Loss: 3.882495\n",
            "average loss per batch in epoch [4/10], Loss: 3.822384\n",
            "average loss per batch in epoch [5/10], Loss: 3.613687\n",
            "average loss per batch in epoch [6/10], Loss: 3.682663\n",
            "average loss per batch in epoch [7/10], Loss: 3.607144\n",
            "average loss per batch in epoch [8/10], Loss: 3.503795\n",
            "average loss per batch in epoch [9/10], Loss: 3.383416\n",
            "average loss per batch in epoch [10/10], Loss: 3.453778\n",
            "---\n",
            "average loss per batch in epoch [1/10], Loss: 4.414052\n",
            "average loss per batch in epoch [2/10], Loss: 4.039785\n",
            "average loss per batch in epoch [3/10], Loss: 3.782329\n",
            "average loss per batch in epoch [4/10], Loss: 3.814222\n",
            "average loss per batch in epoch [5/10], Loss: 3.782018\n",
            "average loss per batch in epoch [6/10], Loss: 3.628115\n",
            "average loss per batch in epoch [7/10], Loss: 3.702206\n",
            "average loss per batch in epoch [8/10], Loss: 3.516008\n",
            "average loss per batch in epoch [9/10], Loss: 3.546184\n",
            "average loss per batch in epoch [10/10], Loss: 3.404303\n",
            "------\n",
            "average batch loss: 1.3910876512527466\n",
            "---\n",
            "average batch loss: 1.3910677433013916\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}